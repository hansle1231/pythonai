{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 학습을 colab 환경에서 진행하였습니다\n",
    "\n",
    "모델생성(112 line) 부분에 max_tar_len과 관련된 문제가있어서 주의해야합니다.\n",
    "\n",
    "설명은 주석으로 달아놓았습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "id": "gJog7uIgnnuP",
    "outputId": "07f356a7-d184-4159-86ee-9fb72acfd1e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xp4Ml5yVIH5X"
   },
   "source": [
    "# 어텐션시도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "colab_type": "code",
    "id": "naM9MMw-jG2c",
    "outputId": "efacf895-4947-4c45-8234-acb4afe4536c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tar</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58052</th>\n",
       "      <td>18325.0</td>\n",
       "      <td>\\t \"내가 무슨 신부닌지는 모르지?\" -끄덕- 또다시 정쩌기 이썯따 \\n</td>\n",
       "      <td>\"내가 무슨 신분인지는 모르지?\" -끄덕- 또다시 정적이 있었다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47467</th>\n",
       "      <td>4158.0</td>\n",
       "      <td>\\t 미사와 호믜 악찔적 수법(사시릐 왜곡) 사차레 기름:미국 거주 일보닌 의사에게...</td>\n",
       "      <td>미사와 홈의 악질적 수법(사실의 왜곡) 사찰에 기름:미국 거주 일본인 의사에게 구속...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109441</th>\n",
       "      <td>17890.0</td>\n",
       "      <td>\\t  제외 상복 남자(양복) 4벌여자상복 5벌 람자:양복,와이셔츠,넥타이포함여자 \\n</td>\n",
       "      <td>제외 상복 남자(양복) 4벌여자상복 5벌 남자:양복,와이셔츠,넥타이포함여자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90735</th>\n",
       "      <td>3576.0</td>\n",
       "      <td>\\t  송장 노리하기는 실커든 \\n</td>\n",
       "      <td>송장 놀이하기는 싫거든</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68013</th>\n",
       "      <td>783.0</td>\n",
       "      <td>\\t 노리학씁 펀트 곤충세트로 잠자리,사마귀 만들면서 어너학쓰 파기 놀아학씁 펀트 ...</td>\n",
       "      <td>놀이학습 펀트 곤충세트로 잠자리,사마귀 만들면서 언어학습 하기 놀아학습 펀트 한글 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17868</th>\n",
       "      <td>23558.0</td>\n",
       "      <td>\\t 적또 읻따니까요~ 이러다 얼렁뚱땅 잠짜리독닙또 하게될까요? 울 아들 이제 다 ...</td>\n",
       "      <td>적도 있다니까요~ 이러다 얼렁뚱땅 잠자리독립도 하게될까요? 울 아들 이제 다 컸다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45207</th>\n",
       "      <td>6256.0</td>\n",
       "      <td>\\t 고궁 등 정쩌긴 풍경을 주로 그리던 도상봉은 1960년 이후에는 물결치는 바다...</td>\n",
       "      <td>고궁 등 정적인 풍경을 주로 그리던 도상봉은 1960년 이후에는 물결치는 바다 등 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54811</th>\n",
       "      <td>10914.0</td>\n",
       "      <td>\\t 것 밖에 못하는 동생을 위해서 잠자리 처럼 하느를 날 개 해 주겓따고 하니 이 \\n</td>\n",
       "      <td>것 밖에 못하는 동생을 위해서 잠자리 처럼 하늘을 날 개 해 주겠다고 하니 이</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>9175.0</td>\n",
       "      <td>\\t 수많은 송짱의 대량 출력또 문제 업껟쬬?! 택빼어븐 빠른 배송을 위해 시간만큼...</td>\n",
       "      <td>수많은 송장의 대량 출력도 문제 없겠죠?! 택배업은 빠른 배송을 위해 시간만큼 중요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44414</th>\n",
       "      <td>5105.0</td>\n",
       "      <td>\\t 마지마그로 와인까지 이날 처으므로 드레스를 입꼬 단상에 올라 상도 받앋씀니다 ...</td>\n",
       "      <td>마지막으로 와인까지 이날 처음으로 드레스를 입고 단상에 올라 상도 받았습니다 상복 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118041 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ...                                                src\n",
       "58052   18325.0  ...                \"내가 무슨 신분인지는 모르지?\" -끄덕- 또다시 정적이 있었다\n",
       "47467    4158.0  ...  미사와 홈의 악질적 수법(사실의 왜곡) 사찰에 기름:미국 거주 일본인 의사에게 구속...\n",
       "109441  17890.0  ...          제외 상복 남자(양복) 4벌여자상복 5벌 남자:양복,와이셔츠,넥타이포함여자\n",
       "90735    3576.0  ...                                       송장 놀이하기는 싫거든\n",
       "68013     783.0  ...  놀이학습 펀트 곤충세트로 잠자리,사마귀 만들면서 언어학습 하기 놀아학습 펀트 한글 ...\n",
       "...         ...  ...                                                ...\n",
       "17868   23558.0  ...      적도 있다니까요~ 이러다 얼렁뚱땅 잠자리독립도 하게될까요? 울 아들 이제 다 컸다\n",
       "45207    6256.0  ...  고궁 등 정적인 풍경을 주로 그리던 도상봉은 1960년 이후에는 물결치는 바다 등 ...\n",
       "54811   10914.0  ...        것 밖에 못하는 동생을 위해서 잠자리 처럼 하늘을 날 개 해 주겠다고 하니 이\n",
       "641      9175.0  ...  수많은 송장의 대량 출력도 문제 없겠죠?! 택배업은 빠른 배송을 위해 시간만큼 중요...\n",
       "44414    5105.0  ...  마지막으로 와인까지 이날 처음으로 드레스를 입고 단상에 올라 상도 받았습니다 상복 ...\n",
       "\n",
       "[118041 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터셋 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 및 모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "0TooD2v9FUGj",
    "outputId": "6357058b-691f-468d-b130-98163debaf31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5123\n",
      "5391\n",
      "[[25, 3103, 3541, 4172, 1, 3827, 3060, 4820, 1, 4340, 4331, 9, 1516, 1530, 10, 1, 4125, 3916, 4236, 1, 3386, 3711, 3582, 1, 3640, 4291, 1, 3912, 4236, 3401, 3336], [1, 4151, 3385, 3974, 3430, 3956, 3318, 1, 4037, 2983, 9, 1579, 796, 10, 2983, 1, 3503, 4156, 4204, 3618, 3685, 3942, 1, 3754, 3933, 4810, 3683, 13, 1, 4704, 3113, 3582, 1, 4293, 4812, 1, 3827, 3582, 3974, 3430, 4291, 3221, 1, 3000, 3754, 4291, 4292, 4288, 1, 4294, 3823, 3621, 1, 3974, 3430, 3956], [3433, 4167, 3365, 1, 3996, 4164, 4291, 1, 3749, 3582, 1, 3049, 4542, 3825, 3612, 4811, 1, 3996, 4164, 4291, 4172, 4229, 63, 63, 1, 4291, 1, 3226, 1, 3763, 4239, 1, 3237, 4236, 4276, 1, 4310, 4305, 3629, 13, 1, 3221, 3842, 1, 3438, 1, 3226, 3000, 3621, 1, 4769, 3318, 1, 3049, 4542, 4277, 1, 4785, 3805, 4810, 3318, 1, 3761, 3780, 4298, 3327, 3336], [1, 3925, 4820, 1, 3793, 3410, 3823, 4675, 1, 4291, 3552, 1, 4642, 1, 3923, 3803, 4291, 1, 4291, 4156, 4402, 1, 3020, 4276, 1, 3103, 1, 3393, 4118], [1, 4043, 4204, 4812, 1, 4407, 4277, 1, 3842, 3588, 4820, 1, 4331, 4276, 1, 3387, 4274, 3582, 1, 4310, 4305, 3629, 3621, 1, 4820, 3037, 4813, 1, 3996, 1, 4300, 3318, 1, 3971, 4156, 4810, 4237, 4027, 13, 1, 3855, 3537, 3761, 1, 2997, 4276, 1, 4581, 3730, 3327, 4728, 1, 4037, 3947, 4277, 1, 3113, 3805, 4331, 4274, 3582]]\n",
      "[[1, 3, 27, 3107, 3623, 3789, 3, 3962, 3059, 5086, 3, 4530, 4520, 11, 1517, 1531, 12, 3, 4315, 4081, 4424, 3, 3458, 3833, 3675, 3, 3749, 4480, 3, 4076, 4424, 3463, 3399, 3, 2], [1, 3, 3, 4341, 3450, 4139, 3498, 4245, 3376, 3, 4203, 3130, 11, 1580, 797, 12, 2984, 3, 3586, 4344, 4391, 3721, 3799, 4108, 3, 3880, 4229, 5075, 3797, 15, 3, 4949, 3118, 3675, 3, 4482, 5077, 3, 3962, 3675, 4139, 3497, 3118, 3249, 3, 3000, 3880, 4480, 4480, 3117, 3, 4484, 3958, 3725, 3, 4139, 3498, 4120, 3, 2], [1, 3, 3501, 4347, 3542, 3, 4161, 4344, 3987, 3, 3875, 3675, 3, 3050, 4750, 3962, 3715, 5076, 3, 4161, 4344, 3987, 4359, 4416, 65, 65, 3, 4480, 3, 3255, 3, 3889, 4427, 3, 3266, 4424, 4466, 3, 4500, 4494, 3733, 15, 3, 3249, 3987, 3, 3506, 3, 3255, 3000, 3725, 3, 5017, 3376, 3, 3050, 4750, 4467, 3, 5041, 3934, 5075, 3376, 3, 3887, 3902, 3993, 3386, 3399, 3, 2], [1, 3, 3, 4090, 5086, 3, 3922, 3578, 3958, 4910, 3, 4480, 3642, 3, 4872, 3, 4088, 4044, 3118, 3, 4480, 4344, 4594, 3, 3023, 4466, 3, 3107, 3, 3458, 4307, 3, 2], [1, 3, 3, 4203, 3810, 5077, 3, 4592, 3982, 3, 3987, 3683, 5086, 3, 4519, 3109, 3, 3450, 3374, 3675, 3, 4500, 4494, 3733, 3725, 3, 5086, 3039, 5079, 3, 4161, 3, 4483, 3376, 3, 4136, 4344, 5075, 4425, 4192, 15, 3, 4003, 3623, 3887, 3, 2984, 4965, 3, 4799, 3852, 3386, 4975, 3, 4203, 4108, 3725, 3, 3118, 3934, 4519, 3107, 3675, 3, 2]]\n",
      "[[3, 27, 3107, 3623, 3789, 3, 3962, 3059, 5086, 3, 4530, 4520, 11, 1517, 1531, 12, 3, 4315, 4081, 4424, 3, 3458, 3833, 3675, 3, 3749, 4480, 3, 4076, 4424, 3463, 3399, 3, 2], [3, 3, 4341, 3450, 4139, 3498, 4245, 3376, 3, 4203, 3130, 11, 1580, 797, 12, 2984, 3, 3586, 4344, 4391, 3721, 3799, 4108, 3, 3880, 4229, 5075, 3797, 15, 3, 4949, 3118, 3675, 3, 4482, 5077, 3, 3962, 3675, 4139, 3497, 3118, 3249, 3, 3000, 3880, 4480, 4480, 3117, 3, 4484, 3958, 3725, 3, 4139, 3498, 4120, 3, 2], [3, 3501, 4347, 3542, 3, 4161, 4344, 3987, 3, 3875, 3675, 3, 3050, 4750, 3962, 3715, 5076, 3, 4161, 4344, 3987, 4359, 4416, 65, 65, 3, 4480, 3, 3255, 3, 3889, 4427, 3, 3266, 4424, 4466, 3, 4500, 4494, 3733, 15, 3, 3249, 3987, 3, 3506, 3, 3255, 3000, 3725, 3, 5017, 3376, 3, 3050, 4750, 4467, 3, 5041, 3934, 5075, 3376, 3, 3887, 3902, 3993, 3386, 3399, 3, 2], [3, 3, 4090, 5086, 3, 3922, 3578, 3958, 4910, 3, 4480, 3642, 3, 4872, 3, 4088, 4044, 3118, 3, 4480, 4344, 4594, 3, 3023, 4466, 3, 3107, 3, 3458, 4307, 3, 2], [3, 3, 4203, 3810, 5077, 3, 4592, 3982, 3, 3987, 3683, 5086, 3, 4519, 3109, 3, 3450, 3374, 3675, 3, 4500, 4494, 3733, 3725, 3, 5086, 3039, 5079, 3, 4161, 3, 4483, 3376, 3, 4136, 4344, 5075, 4425, 4192, 15, 3, 4003, 3623, 3887, 3, 2984, 4965, 3, 4799, 3852, 3386, 4975, 3, 4203, 4108, 3725, 3, 3118, 3934, 4519, 3107, 3675, 3, 2]]\n",
      "228\n",
      "232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "import json\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers, losses, metrics\n",
    "from keras import preprocessing\n",
    "from keras.optimizers import SGD,RMSprop\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "# lines= pd.read_csv('/gdrive/My Drive/for colab/totaldata.csv', names=['','tar', 'src']).drop(0).sample(frac=1)\n",
    "lines= pd.read_csv('./totaldata.csv', names=['','tar', 'src']).drop(0).sample(frac=1) ## 데이터셋 \n",
    "\n",
    "\n",
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "src_vocab=set() \n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    for char in line: # 1개의 글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab=set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)\n",
    "\n",
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)\n",
    "\n",
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "\n",
    "\n",
    "\n",
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "\n",
    "encoder_input = []\n",
    "for line in lines.src: #입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp_X = []\n",
    "    for w in line: #각 줄에서 1개씩 글자를 읽음\n",
    "        temp_X.append(src_to_index[w]) # 글자를 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_X)\n",
    "print(encoder_input[:5])\n",
    "\n",
    "\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])\n",
    "qq=encoder_input[0]\n",
    "\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t>0:\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])\n",
    "\n",
    "\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "#---------------------모델생성-------------------------\n",
    "encoder_inputs = layers.Input(shape=(None, src_vocab_size))  #원핫 \n",
    "\n",
    "#인코더\n",
    "encoder_lstm = layers.GRU(units=256, return_state=True,return_sequences=True)\n",
    "encoder_outputs, state_h = encoder_lstm(encoder_inputs)  #(batch, src_seq,256)\n",
    "encoder_states = state_h\n",
    "encoder_model = models.Model(inputs=[encoder_inputs], outputs=[encoder_outputs, state_h])\n",
    "\n",
    "#디코더\n",
    "decoder_inputs = layers.Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = layers.GRU(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _= decoder_lstm(decoder_inputs, initial_state=encoder_states) # batch ,tar_seq,256 \n",
    "\n",
    "\n",
    "\n",
    "#어텐션\n",
    "dot_layers=layers.Dot(axes=-1)\n",
    "dot=dot_layers([decoder_outputs,encoder_outputs]) #(batch,max_tar,max_src_)\n",
    "softmax_score_layer=layers.Softmax(axis=-1)\n",
    "dot=softmax_score_layer(dot)\n",
    "\n",
    "\n",
    "# !!!!  문제가 되는부분 max_tar_len=232 인데 232를 꼭숫자로 넣어주어야함. max_tar_len으로 넣으면 모델 로드시 문제생김....!!\n",
    "enc_rep_layer=layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x,1),232,1))  #batch,max_tar_len,src_vocab,256\n",
    "enc_rep=enc_rep_layer(encoder_outputs)\n",
    "\n",
    "dot_repeat_layers=layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x,1),256,1)) #(batch,256,tar_seq)\n",
    "dot_repeat=dot_repeat_layers(dot)\n",
    "score_model=models.Model([encoder_inputs,decoder_inputs],dot)\n",
    "dot_transpose=layers.Lambda(lambda x: K.permute_dimensions(x,[0,2,3,1]))(dot_repeat)  #(batch,tarseq,256)\n",
    "\n",
    "\n",
    "mul=layers.Multiply()([dot_transpose,enc_rep]) #batch,tar,256\n",
    "\n",
    "context=layers.Lambda(lambda x : K.sum(x,axis=2,keepdims=False))(mul)\n",
    "\n",
    "\n",
    "attention_outputs=layers.Concatenate(axis=-1)([decoder_outputs,context])\n",
    "\n",
    "decoder_softmax_layer =layers.TimeDistributed(layers.Dense(tar_vocab_size, activation='softmax'))\n",
    "decoder_outputs = decoder_softmax_layer(attention_outputs)\n",
    "\n",
    "model = models.Model([encoder_inputs, decoder_inputs], [decoder_outputs])\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",metrics=['acc'])\n",
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbn9eqwVZ_sp"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#추가학습을위해 객체들 저장\n",
    "\n",
    "with open('./totalpickle.p', 'wb') as file:    # james.p 파일을 바이너리 쓰기 모드(wb)로 열기\n",
    "    pickle.dump(src_vocab, file)\n",
    "    pickle.dump(tar_vocab, file)\n",
    "   \n",
    "    pickle.dump(src_to_index, file)\n",
    "    pickle.dump(tar_to_index, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x76C9I5GTy8X"
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NoTXUcpORpJJ"
   },
   "outputs": [],
   "source": [
    "#재학습시 모델, 객체들 로드\n",
    "import pickle\n",
    "\n",
    "with open('./totalpickle.p', 'rb') as file:    # james.p 파일을 바이너리 쓰기 모드(wb)로 열기\n",
    "    src_vocab= pickle.load(file)\n",
    "    tar_vocab=pickle.load(file)   \n",
    "    src_to_index=pickle.load(file)\n",
    "    tar_to_index=pickle.load(file)\n",
    "\n",
    "\n",
    "model=load_model('./total.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 만약 \n",
    "학습은 건너뛰고 test만 하고자 한다면\n",
    "미리학습된 모델을 load하셔서 사용하시면 됩니다.\n",
    "\n",
    "model=load_model('./total.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mxV4nDXXG21u",
    "outputId": "e1a7732f-7d4a-4a98-c73b-7f726a0109b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 14s 31ms/step - loss: 4.5121 - acc: 0.5002 - val_loss: 1.8775 - val_acc: 0.7427\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.8748 - acc: 0.7048 - val_loss: 1.4857 - val_acc: 0.7712\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.5753 - acc: 0.7452 - val_loss: 1.4797 - val_acc: 0.7459\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.4547 - acc: 0.7584 - val_loss: 1.3216 - val_acc: 0.8026\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.3147 - acc: 0.7981 - val_loss: 1.2696 - val_acc: 0.8023\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.2609 - acc: 0.8056 - val_loss: 1.2903 - val_acc: 0.7972\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.6601 - acc: 0.8028 - val_loss: 1.4246 - val_acc: 0.8072\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.2799 - acc: 0.8099 - val_loss: 1.3144 - val_acc: 0.7939\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.2325 - acc: 0.8066 - val_loss: 1.2781 - val_acc: 0.7938\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.1984 - acc: 0.8085 - val_loss: 1.1864 - val_acc: 0.8171\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.1305 - acc: 0.8180 - val_loss: 1.1582 - val_acc: 0.8144\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.1668 - acc: 0.8130 - val_loss: 1.2011 - val_acc: 0.8067\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.1338 - acc: 0.8156 - val_loss: 1.0065 - val_acc: 0.8296\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.1439 - acc: 0.8109 - val_loss: 1.0351 - val_acc: 0.8253\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.1244 - acc: 0.8114 - val_loss: 1.0712 - val_acc: 0.8185\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.0804 - acc: 0.8185 - val_loss: 1.0548 - val_acc: 0.8168\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.0763 - acc: 0.8170 - val_loss: 1.2021 - val_acc: 0.7933\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9984 - acc: 0.8289 - val_loss: 1.1205 - val_acc: 0.8082\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.0532 - acc: 0.8180 - val_loss: 1.0647 - val_acc: 0.8128\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9750 - acc: 0.8312 - val_loss: 1.0156 - val_acc: 0.8203\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.1099 - acc: 0.8163 - val_loss: 1.3633 - val_acc: 0.8075\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.0001 - acc: 0.8278 - val_loss: 1.0381 - val_acc: 0.8211\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9962 - acc: 0.8264 - val_loss: 1.0124 - val_acc: 0.8210\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.0061 - acc: 0.8226 - val_loss: 0.9798 - val_acc: 0.8299\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9310 - acc: 0.8348 - val_loss: 0.9255 - val_acc: 0.8359\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9426 - acc: 0.8322 - val_loss: 0.8740 - val_acc: 0.8457\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9282 - acc: 0.8357 - val_loss: 0.8449 - val_acc: 0.8475\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9584 - acc: 0.8288 - val_loss: 0.8755 - val_acc: 0.8400\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9378 - acc: 0.8334 - val_loss: 0.9044 - val_acc: 0.8396\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9704 - acc: 0.8257 - val_loss: 0.9142 - val_acc: 0.8366\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9447 - acc: 0.8289 - val_loss: 0.9486 - val_acc: 0.8291\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9278 - acc: 0.8325 - val_loss: 0.8261 - val_acc: 0.8496\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9105 - acc: 0.8339 - val_loss: 0.9038 - val_acc: 0.8353\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9115 - acc: 0.8344 - val_loss: 0.9385 - val_acc: 0.8297\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9041 - acc: 0.8355 - val_loss: 0.8546 - val_acc: 0.8482\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9475 - acc: 0.8269 - val_loss: 0.8747 - val_acc: 0.8414\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9260 - acc: 0.8296 - val_loss: 1.0175 - val_acc: 0.8098\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8998 - acc: 0.8347 - val_loss: 0.9874 - val_acc: 0.8161\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8904 - acc: 0.8356 - val_loss: 0.8009 - val_acc: 0.8509\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.9007 - acc: 0.8335 - val_loss: 0.8085 - val_acc: 0.8522\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8945 - acc: 0.8337 - val_loss: 0.8909 - val_acc: 0.8371\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8666 - acc: 0.8406 - val_loss: 0.8809 - val_acc: 0.8350\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8893 - acc: 0.8343 - val_loss: 0.8401 - val_acc: 0.8457\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8639 - acc: 0.8395 - val_loss: 0.8899 - val_acc: 0.8316\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8652 - acc: 0.8383 - val_loss: 0.8866 - val_acc: 0.8382\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8945 - acc: 0.8333 - val_loss: 0.8817 - val_acc: 0.8386\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.9045 - acc: 0.8322 - val_loss: 0.8304 - val_acc: 0.8449\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8423 - acc: 0.8432 - val_loss: 0.9818 - val_acc: 0.8179\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8796 - acc: 0.8348 - val_loss: 0.9202 - val_acc: 0.8278\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8637 - acc: 0.8377 - val_loss: 0.9432 - val_acc: 0.8258\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8552 - acc: 0.8390 - val_loss: 0.9780 - val_acc: 0.8171\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8327 - acc: 0.8417 - val_loss: 0.9035 - val_acc: 0.8331\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8109 - acc: 0.8481 - val_loss: 0.8004 - val_acc: 0.8483\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8159 - acc: 0.8460 - val_loss: 0.8132 - val_acc: 0.8441\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.8299 - acc: 0.8435 - val_loss: 0.7896 - val_acc: 0.8522\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.7895 - acc: 0.8532 - val_loss: 0.6953 - val_acc: 0.8703\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.7240 - acc: 0.8644 - val_loss: 0.6412 - val_acc: 0.8821\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.6465 - acc: 0.8815 - val_loss: 0.6105 - val_acc: 0.8835\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.5642 - acc: 0.8982 - val_loss: 0.5703 - val_acc: 0.8977\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.4991 - acc: 0.9110 - val_loss: 0.4110 - val_acc: 0.9259\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.4050 - acc: 0.9286 - val_loss: 0.4228 - val_acc: 0.9228\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.3725 - acc: 0.9327 - val_loss: 0.3762 - val_acc: 0.9348\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.3370 - acc: 0.9401 - val_loss: 0.2852 - val_acc: 0.9492\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.3175 - acc: 0.9447 - val_loss: 0.2857 - val_acc: 0.9540\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.3069 - acc: 0.9460 - val_loss: 0.2691 - val_acc: 0.9535\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.2457 - acc: 0.9572 - val_loss: 0.2143 - val_acc: 0.9609\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.2348 - acc: 0.9582 - val_loss: 0.2097 - val_acc: 0.9620\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.2102 - acc: 0.9629 - val_loss: 0.1998 - val_acc: 0.9641\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.2031 - acc: 0.9630 - val_loss: 0.2054 - val_acc: 0.9628\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1706 - acc: 0.9682 - val_loss: 0.1502 - val_acc: 0.9726\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.2070 - acc: 0.9621 - val_loss: 0.1628 - val_acc: 0.9684\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1586 - acc: 0.9697 - val_loss: 0.1782 - val_acc: 0.9675\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1562 - acc: 0.9704 - val_loss: 0.1561 - val_acc: 0.9720\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1653 - acc: 0.9672 - val_loss: 0.2777 - val_acc: 0.9503\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1661 - acc: 0.9686 - val_loss: 0.1402 - val_acc: 0.9715\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1335 - acc: 0.9727 - val_loss: 0.1328 - val_acc: 0.9734\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1281 - acc: 0.9733 - val_loss: 0.1303 - val_acc: 0.9765\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1384 - acc: 0.9723 - val_loss: 0.1421 - val_acc: 0.9694\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1185 - acc: 0.9747 - val_loss: 0.1425 - val_acc: 0.9731\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1273 - acc: 0.9733 - val_loss: 0.1148 - val_acc: 0.9747\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1164 - acc: 0.9748 - val_loss: 0.1216 - val_acc: 0.9729\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1083 - acc: 0.9770 - val_loss: 0.0881 - val_acc: 0.9797\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1141 - acc: 0.9756 - val_loss: 0.1216 - val_acc: 0.9709\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1047 - acc: 0.9768 - val_loss: 0.1067 - val_acc: 0.9777\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0944 - acc: 0.9785 - val_loss: 0.1000 - val_acc: 0.9765\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0918 - acc: 0.9785 - val_loss: 0.1234 - val_acc: 0.9748\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0990 - acc: 0.9771 - val_loss: 0.0905 - val_acc: 0.9781\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1005 - acc: 0.9772 - val_loss: 0.1307 - val_acc: 0.9729\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1014 - acc: 0.9771 - val_loss: 0.1047 - val_acc: 0.9772\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0865 - acc: 0.9793 - val_loss: 0.0899 - val_acc: 0.9795\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0961 - acc: 0.9786 - val_loss: 0.0967 - val_acc: 0.9760\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0872 - acc: 0.9791 - val_loss: 0.1045 - val_acc: 0.9728\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0916 - acc: 0.9786 - val_loss: 0.0987 - val_acc: 0.9793\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0857 - acc: 0.9798 - val_loss: 0.0860 - val_acc: 0.9805\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0893 - acc: 0.9793 - val_loss: 0.0817 - val_acc: 0.9810\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0825 - acc: 0.9800 - val_loss: 0.0850 - val_acc: 0.9794\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0895 - acc: 0.9784 - val_loss: 0.0722 - val_acc: 0.9825\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0890 - acc: 0.9788 - val_loss: 0.1148 - val_acc: 0.9718\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0812 - acc: 0.9803 - val_loss: 0.0920 - val_acc: 0.9786\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0774 - acc: 0.9803 - val_loss: 0.0824 - val_acc: 0.9791\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.8436 - acc: 0.8384 - val_loss: 3.6706 - val_acc: 0.7455\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 2.3508 - acc: 0.7379 - val_loss: 1.6874 - val_acc: 0.7687\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 1.5851 - acc: 0.7831 - val_loss: 1.5039 - val_acc: 0.7539\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 1.1220 - acc: 0.8305 - val_loss: 0.3080 - val_acc: 0.9417\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1823 - acc: 0.9636 - val_loss: 0.1084 - val_acc: 0.9744\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1239 - acc: 0.9730 - val_loss: 0.0946 - val_acc: 0.9781\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.1093 - acc: 0.9753 - val_loss: 0.1085 - val_acc: 0.9759\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0973 - acc: 0.9780 - val_loss: 0.1061 - val_acc: 0.9770\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0868 - acc: 0.9796 - val_loss: 0.1011 - val_acc: 0.9784\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0794 - acc: 0.9806 - val_loss: 0.0642 - val_acc: 0.9832\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0833 - acc: 0.9794 - val_loss: 0.0793 - val_acc: 0.9798\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0818 - acc: 0.9799 - val_loss: 0.0581 - val_acc: 0.9839\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0752 - acc: 0.9812 - val_loss: 0.1345 - val_acc: 0.9741\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0814 - acc: 0.9796 - val_loss: 0.0540 - val_acc: 0.9844\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0731 - acc: 0.9823 - val_loss: 0.0823 - val_acc: 0.9788\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0688 - acc: 0.9820 - val_loss: 0.0599 - val_acc: 0.9842\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0704 - acc: 0.9820 - val_loss: 0.0726 - val_acc: 0.9819\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0740 - acc: 0.9812 - val_loss: 0.0744 - val_acc: 0.9784\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0735 - acc: 0.9814 - val_loss: 0.0791 - val_acc: 0.9793\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0686 - acc: 0.9820 - val_loss: 0.0605 - val_acc: 0.9841\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0727 - acc: 0.9813 - val_loss: 0.0897 - val_acc: 0.9774\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0743 - acc: 0.9811 - val_loss: 0.0629 - val_acc: 0.9841\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0795 - acc: 0.9803 - val_loss: 0.0701 - val_acc: 0.9818\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0675 - acc: 0.9818 - val_loss: 0.0673 - val_acc: 0.9809\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0692 - acc: 0.9818 - val_loss: 0.0879 - val_acc: 0.9803\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0757 - acc: 0.9806 - val_loss: 0.0572 - val_acc: 0.9841\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0634 - acc: 0.9830 - val_loss: 0.0581 - val_acc: 0.9837\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0624 - acc: 0.9831 - val_loss: 0.0584 - val_acc: 0.9834\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0725 - acc: 0.9810 - val_loss: 0.0908 - val_acc: 0.9763\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0655 - acc: 0.9820 - val_loss: 0.0671 - val_acc: 0.9818\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0661 - acc: 0.9826 - val_loss: 0.0597 - val_acc: 0.9844\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0678 - acc: 0.9822 - val_loss: 0.0701 - val_acc: 0.9828\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0701 - acc: 0.9811 - val_loss: 0.0595 - val_acc: 0.9847\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0656 - acc: 0.9825 - val_loss: 0.0739 - val_acc: 0.9795\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0739 - acc: 0.9809 - val_loss: 0.0625 - val_acc: 0.9826\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0648 - acc: 0.9825 - val_loss: 0.0467 - val_acc: 0.9853\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0597 - acc: 0.9831 - val_loss: 0.0813 - val_acc: 0.9812\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0714 - acc: 0.9815 - val_loss: 0.0668 - val_acc: 0.9816\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0607 - acc: 0.9837 - val_loss: 0.0683 - val_acc: 0.9822\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0609 - acc: 0.9829 - val_loss: 0.0806 - val_acc: 0.9797\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0589 - acc: 0.9840 - val_loss: 0.0541 - val_acc: 0.9848\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0638 - acc: 0.9825 - val_loss: 0.0485 - val_acc: 0.9853\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0620 - acc: 0.9836 - val_loss: 0.0609 - val_acc: 0.9833\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0601 - acc: 0.9837 - val_loss: 0.0453 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0613 - acc: 0.9830 - val_loss: 0.0716 - val_acc: 0.9824\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0628 - acc: 0.9826 - val_loss: 0.0653 - val_acc: 0.9829\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0621 - acc: 0.9831 - val_loss: 0.0468 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0658 - acc: 0.9821 - val_loss: 0.0554 - val_acc: 0.9845\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0596 - acc: 0.9838 - val_loss: 0.0698 - val_acc: 0.9829\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0615 - acc: 0.9837 - val_loss: 0.0645 - val_acc: 0.9828\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0525 - acc: 0.9845 - val_loss: 0.0672 - val_acc: 0.9831\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0607 - acc: 0.9837 - val_loss: 0.0523 - val_acc: 0.9841\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0641 - acc: 0.9828 - val_loss: 0.0641 - val_acc: 0.9808\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0585 - acc: 0.9838 - val_loss: 0.0572 - val_acc: 0.9833\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0615 - acc: 0.9836 - val_loss: 0.0589 - val_acc: 0.9829\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0574 - acc: 0.9838 - val_loss: 0.0488 - val_acc: 0.9849\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0586 - acc: 0.9843 - val_loss: 0.0521 - val_acc: 0.9845\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0534 - acc: 0.9848 - val_loss: 0.0585 - val_acc: 0.9843\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0542 - acc: 0.9841 - val_loss: 0.0853 - val_acc: 0.9801\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0567 - acc: 0.9839 - val_loss: 0.0614 - val_acc: 0.9844\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0620 - acc: 0.9830 - val_loss: 0.0567 - val_acc: 0.9838\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0564 - acc: 0.9841 - val_loss: 0.0537 - val_acc: 0.9844\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0604 - acc: 0.9835 - val_loss: 0.0612 - val_acc: 0.9842\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0574 - acc: 0.9837 - val_loss: 0.0458 - val_acc: 0.9856\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0593 - acc: 0.9839 - val_loss: 0.0491 - val_acc: 0.9855\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0528 - acc: 0.9844 - val_loss: 0.0622 - val_acc: 0.9838\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0496 - acc: 0.9858 - val_loss: 0.0682 - val_acc: 0.9813\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0549 - acc: 0.9850 - val_loss: 0.0625 - val_acc: 0.9831\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0582 - acc: 0.9841 - val_loss: 0.0368 - val_acc: 0.9882\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0582 - acc: 0.9836 - val_loss: 0.0565 - val_acc: 0.9828\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0573 - acc: 0.9840 - val_loss: 0.0608 - val_acc: 0.9842\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0586 - acc: 0.9841 - val_loss: 0.0623 - val_acc: 0.9849\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0495 - acc: 0.9858 - val_loss: 0.0574 - val_acc: 0.9845\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0523 - acc: 0.9850 - val_loss: 0.0545 - val_acc: 0.9856\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0572 - acc: 0.9847 - val_loss: 0.0606 - val_acc: 0.9832\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0607 - acc: 0.9842 - val_loss: 0.0434 - val_acc: 0.9882\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0579 - acc: 0.9839 - val_loss: 0.0506 - val_acc: 0.9837\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0514 - acc: 0.9854 - val_loss: 0.0885 - val_acc: 0.9796\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0570 - acc: 0.9839 - val_loss: 0.0573 - val_acc: 0.9846\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0543 - acc: 0.9850 - val_loss: 0.0791 - val_acc: 0.9809\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.5252 - acc: 0.9088 - val_loss: 0.0511 - val_acc: 0.9860\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0625 - acc: 0.9839 - val_loss: 0.0422 - val_acc: 0.9869\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0564 - acc: 0.9845 - val_loss: 0.0470 - val_acc: 0.9856\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0537 - acc: 0.9849 - val_loss: 0.0623 - val_acc: 0.9837\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0513 - acc: 0.9853 - val_loss: 0.0598 - val_acc: 0.9816\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0499 - acc: 0.9855 - val_loss: 0.0410 - val_acc: 0.9877\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0551 - acc: 0.9848 - val_loss: 0.0525 - val_acc: 0.9846\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0479 - acc: 0.9860 - val_loss: 0.0595 - val_acc: 0.9836\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0563 - acc: 0.9849 - val_loss: 0.0443 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0503 - acc: 0.9858 - val_loss: 0.0490 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0476 - acc: 0.9857 - val_loss: 0.0598 - val_acc: 0.9843\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0562 - acc: 0.9840 - val_loss: 0.0664 - val_acc: 0.9816\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0610 - acc: 0.9839 - val_loss: 0.0461 - val_acc: 0.9851\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0536 - acc: 0.9851 - val_loss: 0.0847 - val_acc: 0.9784\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0568 - acc: 0.9843 - val_loss: 0.0451 - val_acc: 0.9864\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0539 - acc: 0.9854 - val_loss: 0.0627 - val_acc: 0.9820\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0574 - acc: 0.9843 - val_loss: 0.0382 - val_acc: 0.9877\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0509 - acc: 0.9851 - val_loss: 0.0412 - val_acc: 0.9882\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0474 - acc: 0.9854 - val_loss: 0.0550 - val_acc: 0.9847\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0559 - acc: 0.9847 - val_loss: 0.0570 - val_acc: 0.9823\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0556 - acc: 0.9847 - val_loss: 0.0606 - val_acc: 0.9834\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0465 - acc: 0.9865 - val_loss: 0.0521 - val_acc: 0.9861\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0479 - acc: 0.9860 - val_loss: 0.0643 - val_acc: 0.9854\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0558 - acc: 0.9844 - val_loss: 0.0477 - val_acc: 0.9854\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0516 - acc: 0.9853 - val_loss: 0.0423 - val_acc: 0.9883\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0520 - acc: 0.9853 - val_loss: 0.0456 - val_acc: 0.9863\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0455 - acc: 0.9864 - val_loss: 0.0450 - val_acc: 0.9866\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0518 - acc: 0.9852 - val_loss: 0.0537 - val_acc: 0.9841\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0449 - acc: 0.9867 - val_loss: 0.0377 - val_acc: 0.9880\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0470 - acc: 0.9864 - val_loss: 0.0456 - val_acc: 0.9861\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0542 - acc: 0.9853 - val_loss: 0.0371 - val_acc: 0.9877\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0470 - acc: 0.9859 - val_loss: 0.0447 - val_acc: 0.9865\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0493 - acc: 0.9858 - val_loss: 0.1144 - val_acc: 0.9804\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0499 - acc: 0.9857 - val_loss: 0.0380 - val_acc: 0.9891\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0514 - acc: 0.9853 - val_loss: 0.0546 - val_acc: 0.9839\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0512 - acc: 0.9857 - val_loss: 0.0437 - val_acc: 0.9870\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0570 - acc: 0.9847 - val_loss: 0.0424 - val_acc: 0.9861\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0570 - acc: 0.9836 - val_loss: 0.0420 - val_acc: 0.9862\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0440 - acc: 0.9868 - val_loss: 0.0527 - val_acc: 0.9850\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0494 - acc: 0.9856 - val_loss: 0.0629 - val_acc: 0.9831\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0466 - acc: 0.9863 - val_loss: 0.0483 - val_acc: 0.9865\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0455 - acc: 0.9862 - val_loss: 0.0422 - val_acc: 0.9863\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0476 - acc: 0.9860 - val_loss: 0.0487 - val_acc: 0.9853\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0503 - acc: 0.9863 - val_loss: 0.0491 - val_acc: 0.9853\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0483 - acc: 0.9859 - val_loss: 0.0508 - val_acc: 0.9846\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0514 - acc: 0.9848 - val_loss: 0.0408 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0497 - acc: 0.9856 - val_loss: 0.0421 - val_acc: 0.9873\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0481 - acc: 0.9853 - val_loss: 0.0480 - val_acc: 0.9859\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0532 - acc: 0.9847 - val_loss: 0.0546 - val_acc: 0.9845\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0510 - acc: 0.9861 - val_loss: 0.0567 - val_acc: 0.9851\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0451 - acc: 0.9859 - val_loss: 0.0410 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0454 - acc: 0.9869 - val_loss: 0.0343 - val_acc: 0.9901\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0449 - acc: 0.9863 - val_loss: 0.0468 - val_acc: 0.9858\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0463 - acc: 0.9865 - val_loss: 0.0573 - val_acc: 0.9857\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0483 - acc: 0.9864 - val_loss: 0.0458 - val_acc: 0.9867\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0480 - acc: 0.9861 - val_loss: 0.0411 - val_acc: 0.9874\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0481 - acc: 0.9865 - val_loss: 0.0511 - val_acc: 0.9867\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0558 - acc: 0.9847 - val_loss: 0.0353 - val_acc: 0.9879\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0449 - acc: 0.9865 - val_loss: 0.0542 - val_acc: 0.9843\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0510 - acc: 0.9863 - val_loss: 0.0528 - val_acc: 0.9841\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0472 - acc: 0.9866 - val_loss: 0.0362 - val_acc: 0.9890\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0453 - acc: 0.9860 - val_loss: 0.0430 - val_acc: 0.9861\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0455 - acc: 0.9866 - val_loss: 0.0491 - val_acc: 0.9857\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0448 - acc: 0.9865 - val_loss: 0.0458 - val_acc: 0.9857\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0470 - acc: 0.9867 - val_loss: 0.0394 - val_acc: 0.9873\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0498 - acc: 0.9855 - val_loss: 0.0555 - val_acc: 0.9857\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0410 - acc: 0.9875 - val_loss: 0.0387 - val_acc: 0.9890\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0508 - acc: 0.9860 - val_loss: 0.0593 - val_acc: 0.9833\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0457 - acc: 0.9864 - val_loss: 0.0337 - val_acc: 0.9891\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0440 - acc: 0.9869 - val_loss: 0.0355 - val_acc: 0.9877\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0448 - acc: 0.9867 - val_loss: 0.0442 - val_acc: 0.9867\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0469 - acc: 0.9862 - val_loss: 0.0333 - val_acc: 0.9891\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0452 - acc: 0.9866 - val_loss: 0.0457 - val_acc: 0.9865\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0443 - acc: 0.9872 - val_loss: 0.0554 - val_acc: 0.9846\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0456 - acc: 0.9860 - val_loss: 0.0606 - val_acc: 0.9833\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0460 - acc: 0.9864 - val_loss: 0.0538 - val_acc: 0.9859\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0461 - acc: 0.9866 - val_loss: 0.0448 - val_acc: 0.9862\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0457 - acc: 0.9867 - val_loss: 0.0461 - val_acc: 0.9875\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0460 - acc: 0.9865 - val_loss: 0.0404 - val_acc: 0.9870\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0474 - acc: 0.9859 - val_loss: 0.0621 - val_acc: 0.9859\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0420 - acc: 0.9875 - val_loss: 0.0358 - val_acc: 0.9879\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0461 - acc: 0.9865 - val_loss: 0.0411 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0444 - acc: 0.9866 - val_loss: 0.0375 - val_acc: 0.9891\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0431 - acc: 0.9870 - val_loss: 0.0472 - val_acc: 0.9851\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0439 - acc: 0.9872 - val_loss: 0.0410 - val_acc: 0.9883\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0452 - acc: 0.9868 - val_loss: 0.0526 - val_acc: 0.9832\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0454 - acc: 0.9872 - val_loss: 0.0410 - val_acc: 0.9881\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0691 - acc: 0.9796 - val_loss: 0.0333 - val_acc: 0.9889\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0494 - acc: 0.9857 - val_loss: 0.0360 - val_acc: 0.9884\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0477 - acc: 0.9861 - val_loss: 0.0430 - val_acc: 0.9859\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0461 - acc: 0.9867 - val_loss: 0.0421 - val_acc: 0.9874\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0517 - acc: 0.9856 - val_loss: 0.0479 - val_acc: 0.9862\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0484 - acc: 0.9857 - val_loss: 0.0675 - val_acc: 0.9831\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0430 - acc: 0.9871 - val_loss: 0.0451 - val_acc: 0.9860\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0399 - acc: 0.9875 - val_loss: 0.0294 - val_acc: 0.9912\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0414 - acc: 0.9874 - val_loss: 0.0440 - val_acc: 0.9896\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0454 - acc: 0.9867 - val_loss: 0.0417 - val_acc: 0.9878\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0455 - acc: 0.9868 - val_loss: 0.0420 - val_acc: 0.9861\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0435 - acc: 0.9867 - val_loss: 0.0439 - val_acc: 0.9875\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0441 - acc: 0.9873 - val_loss: 0.0376 - val_acc: 0.9863\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0435 - acc: 0.9865 - val_loss: 0.0457 - val_acc: 0.9870\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0457 - acc: 0.9864 - val_loss: 0.0424 - val_acc: 0.9878\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0483 - acc: 0.9859 - val_loss: 0.0464 - val_acc: 0.9856\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0449 - acc: 0.9866 - val_loss: 0.0474 - val_acc: 0.9870\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0438 - acc: 0.9870 - val_loss: 0.0431 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0436 - acc: 0.9869 - val_loss: 0.0537 - val_acc: 0.9847\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0442 - acc: 0.9868 - val_loss: 0.0476 - val_acc: 0.9848\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0403 - acc: 0.9880 - val_loss: 0.0398 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0414 - acc: 0.9878 - val_loss: 0.0328 - val_acc: 0.9893\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0459 - acc: 0.9869 - val_loss: 0.0343 - val_acc: 0.9897\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0424 - acc: 0.9874 - val_loss: 0.0457 - val_acc: 0.9865\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0454 - acc: 0.9865 - val_loss: 0.0410 - val_acc: 0.9876\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0428 - acc: 0.9867 - val_loss: 0.0369 - val_acc: 0.9888\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0457 - acc: 0.9864 - val_loss: 0.0396 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0447 - acc: 0.9870 - val_loss: 0.0464 - val_acc: 0.9861\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0461 - acc: 0.9863 - val_loss: 0.0364 - val_acc: 0.9878\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0440 - acc: 0.9871 - val_loss: 0.0398 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 23ms/step - loss: 0.0399 - acc: 0.9875 - val_loss: 0.0470 - val_acc: 0.9875\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0451 - acc: 0.9875 - val_loss: 0.0344 - val_acc: 0.9888\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0453 - acc: 0.9868 - val_loss: 0.0403 - val_acc: 0.9883\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0395 - acc: 0.9876 - val_loss: 0.0396 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0430 - acc: 0.9867 - val_loss: 0.0473 - val_acc: 0.9854\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0433 - acc: 0.9870 - val_loss: 0.0363 - val_acc: 0.9886\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0415 - acc: 0.9883 - val_loss: 0.0380 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0409 - acc: 0.9879 - val_loss: 0.0499 - val_acc: 0.9867\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0397 - acc: 0.9878 - val_loss: 0.0325 - val_acc: 0.9900\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0475 - acc: 0.9868 - val_loss: 0.0430 - val_acc: 0.9879\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0402 - acc: 0.9877 - val_loss: 0.0501 - val_acc: 0.9858\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0436 - acc: 0.9869 - val_loss: 0.0356 - val_acc: 0.9879\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0456 - acc: 0.9864 - val_loss: 0.0739 - val_acc: 0.9841\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0448 - acc: 0.9875 - val_loss: 0.0384 - val_acc: 0.9890\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0419 - acc: 0.9871 - val_loss: 0.0496 - val_acc: 0.9856\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0400 - acc: 0.9876 - val_loss: 0.0545 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0476 - acc: 0.9864 - val_loss: 0.0499 - val_acc: 0.9850\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0397 - acc: 0.9876 - val_loss: 0.0587 - val_acc: 0.9853\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0415 - acc: 0.9878 - val_loss: 0.0368 - val_acc: 0.9890\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0400 - acc: 0.9879 - val_loss: 0.0515 - val_acc: 0.9844\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0410 - acc: 0.9880 - val_loss: 0.0320 - val_acc: 0.9891\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0455 - acc: 0.9864 - val_loss: 0.0430 - val_acc: 0.9843\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0414 - acc: 0.9877 - val_loss: 0.0469 - val_acc: 0.9880\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0365 - acc: 0.9886 - val_loss: 0.0418 - val_acc: 0.9852\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0376 - acc: 0.9883 - val_loss: 0.0463 - val_acc: 0.9875\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0385 - acc: 0.9878 - val_loss: 0.0415 - val_acc: 0.9868\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0439 - acc: 0.9870 - val_loss: 0.0590 - val_acc: 0.9822\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0436 - acc: 0.9872 - val_loss: 0.0482 - val_acc: 0.9878\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0387 - acc: 0.9881 - val_loss: 0.0466 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0427 - acc: 0.9873 - val_loss: 0.0483 - val_acc: 0.9847\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0408 - acc: 0.9877 - val_loss: 0.0489 - val_acc: 0.9849\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0431 - acc: 0.9878 - val_loss: 0.0534 - val_acc: 0.9868\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0434 - acc: 0.9871 - val_loss: 0.0402 - val_acc: 0.9884\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0439 - acc: 0.9873 - val_loss: 0.0338 - val_acc: 0.9896\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0395 - acc: 0.9881 - val_loss: 0.0372 - val_acc: 0.9890\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0413 - acc: 0.9876 - val_loss: 0.0341 - val_acc: 0.9902\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0428 - acc: 0.9872 - val_loss: 0.0520 - val_acc: 0.9852\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0395 - acc: 0.9882 - val_loss: 0.0451 - val_acc: 0.9874\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0389 - acc: 0.9879 - val_loss: 0.0441 - val_acc: 0.9870\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0377 - acc: 0.9883 - val_loss: 0.0337 - val_acc: 0.9891\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0402 - acc: 0.9872 - val_loss: 0.0560 - val_acc: 0.9849\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 25ms/step - loss: 0.0384 - acc: 0.9882 - val_loss: 0.0450 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0428 - acc: 0.9872 - val_loss: 0.0419 - val_acc: 0.9864\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0383 - acc: 0.9884 - val_loss: 0.0328 - val_acc: 0.9898\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 10s 23ms/step - loss: 0.0421 - acc: 0.9876 - val_loss: 0.0382 - val_acc: 0.9887\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0388 - acc: 0.9878 - val_loss: 0.0446 - val_acc: 0.9863\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0424 - acc: 0.9877 - val_loss: 0.0578 - val_acc: 0.9858\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0406 - acc: 0.9877 - val_loss: 0.0516 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0381 - acc: 0.9881 - val_loss: 0.0343 - val_acc: 0.9890\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0422 - acc: 0.9874 - val_loss: 0.0369 - val_acc: 0.9889\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0384 - acc: 0.9880 - val_loss: 0.0278 - val_acc: 0.9905\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0396 - acc: 0.9879 - val_loss: 0.0894 - val_acc: 0.9815\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0449 - acc: 0.9871 - val_loss: 0.0299 - val_acc: 0.9898\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0408 - acc: 0.9882 - val_loss: 0.0476 - val_acc: 0.9841\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0384 - acc: 0.9883 - val_loss: 0.0360 - val_acc: 0.9886\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0379 - acc: 0.9886 - val_loss: 0.0392 - val_acc: 0.9874\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0401 - acc: 0.9879 - val_loss: 0.0382 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0406 - acc: 0.9884 - val_loss: 0.0515 - val_acc: 0.9844\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0378 - acc: 0.9884 - val_loss: 0.0351 - val_acc: 0.9894\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0390 - acc: 0.9882 - val_loss: 0.0458 - val_acc: 0.9875\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0417 - acc: 0.9880 - val_loss: 0.0344 - val_acc: 0.9900\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0458 - acc: 0.9874 - val_loss: 0.0426 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0386 - acc: 0.9879 - val_loss: 0.0410 - val_acc: 0.9871\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0406 - acc: 0.9882 - val_loss: 0.0529 - val_acc: 0.9872\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0444 - acc: 0.9876 - val_loss: 0.0316 - val_acc: 0.9896\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0361 - acc: 0.9888 - val_loss: 0.0358 - val_acc: 0.9888\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0356 - acc: 0.9888 - val_loss: 0.0362 - val_acc: 0.9879\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0438 - acc: 0.9869 - val_loss: 0.0566 - val_acc: 0.9844\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "450/450 [==============================] - 11s 24ms/step - loss: 0.0376 - acc: 0.9882 - val_loss: 0.0411 - val_acc: 0.9882\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/1\n",
      "300/450 [===================>..........] - ETA: 3s - loss: 0.0383 - acc: 0.9884"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop,SGD\n",
    "from keras.models import load_model\n",
    "import time\n",
    "sgd=SGD(lr=0.005)\n",
    "rms=RMSprop(lr=0.004)\n",
    "model.compile(optimizer=rms, loss=\"categorical_crossentropy\",metrics=['acc'])\n",
    "# history=model.fit(x=[encoder_input, decoder_input], y=decoder_target, batch_size=50, epochs=30, validation_split=0.1)\n",
    "gc.collect()\n",
    "\n",
    "#한번에학습이 안되므로 500개씩학습\n",
    "#batch를 조절해서 한번에 몇개씩 학습할지 정할수 있다.\n",
    "for epoch in range(10):\n",
    "    bat=500\n",
    "    for i in range(int(len(encoder_input)/bat)):\n",
    "        if i==int(len(encoder_input)/bat):\n",
    "          \n",
    "            encoder_input_=encoder_input[i*bat:]\n",
    "            decoder_input_=decoder_input[i*bat:]\n",
    "            decoder_target_=decoder_target[i*bat:]\n",
    "\n",
    "        else:\n",
    "\n",
    "            encoder_input_=encoder_input[i*bat:(i+1)*bat]\n",
    "            decoder_input_=decoder_input[i*bat:(i+1)*bat]\n",
    "            decoder_target_=decoder_target[i*bat:(i+1)*bat]\n",
    "\n",
    "        \n",
    "        encoder_input_=to_categorical(encoder_input_,src_vocab_size)\n",
    "        decoder_input_=to_categorical(decoder_input_,tar_vocab_size)\n",
    "        decoder_target_=to_categorical(decoder_target_,tar_vocab_size)\n",
    "        model.fit(x=[encoder_input_, decoder_input_], y=decoder_target_, batch_size=50, epochs=1, validation_split=0.1)\n",
    "\n",
    "        del encoder_input_, decoder_input_,decoder_target_\n",
    "\n",
    "        gc.collect()\n",
    "        \n",
    "        model.save('/gdrive/My Drive/for colab/total.h5')\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "opCtd4SaYxNR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOuPj3yQiG-6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "GoUM9BR6knLI",
    "outputId": "00a68ff4-1734-4a62-f9d5-a90b514c0d6e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAEKCAYAAAC2bZqoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGNM2LHkZIwxJuz8PxSfwA4bS2OIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#acc/loss그래프 - 나눠서 학습하는경우에는 큰의미가 없다.\n",
    "  %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(history.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(history.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68RSu-F3OWfK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder model 및 추론모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSZm6bwRbpAs"
   },
   "outputs": [],
   "source": [
    "#인코더모델-학습된 레이어를 가져온다\n",
    "encoder_inputs = layers.Input(shape=(None, src_vocab_size))  #원핫 \n",
    "\n",
    "\n",
    "encoder_lstm = model.layers[2]\n",
    "encoder_outputs, state_h = encoder_lstm(encoder_inputs)  #(batch, src_seq,256)\n",
    "encoder_states = state_h\n",
    "encoder_model = models.Model(inputs=[encoder_inputs], outputs=[encoder_outputs, state_h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgHoFK3bBdh0"
   },
   "outputs": [],
   "source": [
    "#추론모델-학습된 레이어를 가져온다\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "decoder_state_input_h = layers.Input(shape=(256,))\n",
    "decoder_states_inputs = decoder_state_input_h\n",
    "\n",
    "\n",
    "encoder_outputs_input = layers.Input(shape=(None,256))   #(1,src_seq,256)\n",
    "\n",
    "\n",
    "decoder_inputs = layers.Input(shape=(None,tar_vocab_size))#  1,1,tar_vocab_size\n",
    "\n",
    "\n",
    "decoder_outputs, state_h = model.layers[3](decoder_inputs, initial_state=decoder_states_inputs)  #1,1,256\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태를 이전 상태로 사용\n",
    "decoder_states =state_h\n",
    "# 이번에는 훈련 과정에서와 달리 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "\n",
    "test=models.Model([decoder_inputs,decoder_state_input_h],[decoder_outputs])\n",
    "\n",
    "# enc_rep=layers.Lambda(lambda x :K.repeatencoder_outputs_input) #1, tar_seq, src_seq, 256\n",
    "\n",
    "enc_rep=layers.Lambda(lambda x: K.repeat_elements(K.expand_dims(x,1),1,1))(encoder_outputs_input)\n",
    "\n",
    "dot=dot_layers([decoder_outputs,encoder_outputs_input]) #(1,1,256) X (1,src_seq,256)  = (1,1,src_seq)\n",
    "softmax_score_layer=layers.Softmax(axis=2)\n",
    "dot=softmax_score_layer(dot)\n",
    "\n",
    "\n",
    "dot_repeat=dot_repeat_layers(dot)       #(1,256,1,src_seq)\n",
    "\n",
    "dot_transpose=layers.Lambda(lambda x: K.permute_dimensions(x,[0,2,3,1]))(dot_repeat)  #(1,1,src_seq,256)\n",
    "\n",
    "mul=layers.Multiply()([dot_transpose,enc_rep])   #1,1,src_seq,256\n",
    "\n",
    "context=layers.Lambda(lambda x : K.sum(x,axis=2,keepdims=False))(mul)   #(1,1,256)\n",
    "\n",
    "\n",
    "attention_outputs=layers.Concatenate(axis=-1)([decoder_outputs,context])\n",
    "\n",
    "decoder_outputs = model.layers[-1](attention_outputs)\n",
    "decoder_model = models.Model(inputs=[decoder_inputs,encoder_outputs_input,decoder_state_input_h], outputs=[decoder_outputs,decoder_states])\n",
    "index_to_src = dict(\n",
    "    (i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict(\n",
    "    (i, char) for char, i in tar_to_index.items())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "DNb4PQz4pkDI",
    "outputId": "b0a6e541-8630-4b5c-977a-945e1f5e826e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장:  새해 벽두부터 이런 큰 상복이 이어진 것은 그 동안\n",
      "정답 문장:   새해 벽뚜부터 이런 큰 상뽀기 이어진 것은 그 동안 \n",
      "번역기가 번역한 문장:   새해 벼두부터 이런 큰 상뽀기 이어진 것은 그 동안 \n",
      "-----------------------------------\n",
      "입력 문장: 하시는데요^^ 상복대여라든가 리무진제공, 입관용품등을 제공하는 상조회사와는 다르게 보험회사에서는 장례비용자체를 지원합니다\n",
      "정답 문장:  하시는데요^^ 상복대여라든가 리무진제공, 입꽌용품등을 제공하는 상조회사와는 다르게 보험회사에서는 장녜비용자체를 지원합니다 \n",
      "번역기가 번역한 문장:  하시는데요^^ 상뽁대여라든가 리무진제공, 입꽌용품등을 제공하는 상조회사와는 다르게 보험회사에서는 장녜비용자체를 지원합니다 \n",
      "-----------------------------------\n",
      "입력 문장: 소심하게 송장을 달라고 해서 추가추가하니까 아저씨가 송장을\n",
      "정답 문장:  소심하게 송짱을 달라고 해서 추가추가하니까 아저씨가 송짱을 \n",
      "번역기가 번역한 문장:  소심하게 송짱을 달라고 해서 추가추가하니까 아저씨가 송짱을 \n",
      "-----------------------------------\n",
      "입력 문장: 운동) (밴드를 이용한 발목운동) 발목 가동성 운동 안정화 운동 정적 안정화 동적 안정화 동적인 발목운동 일상생활에서 일어날 수 있는 다양한 상황에서의 운동들\n",
      "정답 문장:  운동) (밴드를 이용한 발목운동) 발목 가동성 운동 안정화 운동 정쩍 안정화 동적 안정화 동저긴 발목운동 일쌍생화레서 이러날 수 읻는 다양한 상황에서의 운동들 \n",
      "번역기가 번역한 문장:  운동) (밴드를 이용한 발목운동) 발목 가동성 운동 정쩍 안정화 동적 안정화 동저긴 발목운동저긴 발목운동 일쌍생활에서 이러날 수 인는 다양한 상황에서의 운동드 \n",
      "-----------------------------------\n",
      "입력 문장: ‘쿠션성능시험실’은 최대 500kg 부하가 가능한 로봇, 정하중 시험기, 시트 특성 시험기 등을 사용해 정적하중, 동적하중, 측면지지 강도, 착좌 유지력, 장시간 주행 탄성 유지력를 측정하고 다차원적 분석을\n",
      "정답 문장:  ‘쿠션성능시험시’른 최대 500kg 부하가 가능할 로볻, 정하중 시험기, 시트 특썽 시험기 등을 사용해 정쩌카중, 동저카중, 층면지지 강도, 착쫘 유지력, 장시간 주행 탄성 유지력늘 측쩡하고 다차원적 분서글 \n",
      "번역기가 번역한 문장:  ‘쿠션성능시험실’은 최대 500kg 부하가 가능한 로보, 정하중 시험기, 시트 특썽 시험기 등을 사용해 정쩌카중, 동저카중, 측면지지 강도, 차좌 유지력, 장시간 주행 탄성 유지력를 측쩡하고 다차원저 분석을 \n"
     ]
    }
   ],
   "source": [
    "#텍스트 decode\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    b,states_value = encoder_model.predict(input_seq)\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition: #stop_condition이 True가 될 때까지 루프 반복\n",
    "        output_tokens, h = decoder_model.predict([target_seq, b,states_value])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <sos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트 합니다.\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        states_value = h\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,105]: # 입력 문장의 인덱스\n",
    "    encoder_input_=to_categorical(encoder_input[:1000],src_vocab_size)\n",
    "    input_seq = encoder_input_[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src.iloc[seq_index])\n",
    "    print('정답 문장:', lines.tar.iloc[seq_index][1:len(lines.tar.iloc[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실제 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9_zXRIpwrON"
   },
   "outputs": [],
   "source": [
    "#실제 테스트\n",
    "def make_text(sentence):\n",
    "    index=[]\n",
    "    for i in sentence:\n",
    "        index.append(src_to_index[i])\n",
    "    index=index=pad_sequences([index], maxlen=max_src_len, padding='post')\n",
    "  \n",
    "    index=index[0]\n",
    "    one_hot_data = np.zeros((1, max_src_len, src_vocab_size))\n",
    "\n",
    "  \n",
    "      \n",
    "    for j, ind in enumerate(index):\n",
    "      \n",
    "        one_hot_data[0,j, ind] = 1       \n",
    "    index= one_hot_data\n",
    "\n",
    "    index=decode_sequence(index)\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "yM2-2zFycILg",
    "outputId": "4aa75cd7-a053-4e5d-db2f-0e30a2703783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 상보글 입는꾸 \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_text('상복을 입는꿈')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtDvMlHN-sRy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 반품시 택빼에 송짱을 제거하지 말아주세요 \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_text('반품시 택배에 송장을 제거하지 말아주세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 그는 거의 산 송장이나 다르 멉썯따 \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_text('그는 거의 산 송장이나 다름 없었다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 아캥을 행하면 반드시 대가가 따른다 \\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_text('악행을 행하면 반드시 대가가 따른다')  #miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 숙면을 위해선 잠짜리가 중요하다 \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_text('숙면을 위해선 잠자리가 중요하다') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션 검증-수정필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "x10NfOW7zbj0",
    "outputId": "a9abf4f9-67c2-444c-fe61-2d7d4e5e8d12"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 228 into shape (1,228,5123)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-72c9cdaf9be8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_data_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtest_enc_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_data_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_src_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_vocab_size\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest_dec_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_data_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_tar_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar_vocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 228 into shape (1,228,5123)"
     ]
    }
   ],
   "source": [
    "#어텐션 검증-수정필요\n",
    "test_data_num=35\n",
    "\n",
    "test_enc_input = encoder_input[test_data_num].reshape( (1, max_src_len, src_vocab_size )) \n",
    "test_dec_input = decoder_input[test_data_num].reshape( (1, max_tar_len, tar_vocab_size)) \n",
    "\n",
    "\n",
    "\n",
    "score_values=score_model.predict([test_enc_input,\ttest_dec_input])\n",
    "\n",
    "score_values = score_values.reshape((len(test_dec_input[0]), len(test_enc_input[0]))) \n",
    "# score_values = score_values[:len(x_encoder_[0]), :len(x_decoder_[0])] \n",
    "\n",
    "\n",
    "print('58')\n",
    "path = '/Library/Fonts/NanumBarunpenRegular.otf'\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,15)) \n",
    "ax = fig.add_subplot(111) \n",
    "cax = ax.matshow(score_values, interpolation='nearest') \n",
    "fig.colorbar(cax) \n",
    "test_enc_names = [] \n",
    "for vec in test_enc_input[0]: \n",
    "\tsampled_token_index = np.argmax(vec) \n",
    "\tsampled_char = index_to_src[sampled_token_index] \n",
    "\ttest_enc_names.append(sampled_char) \n",
    "test_dec_names = [] \n",
    "for vec in test_dec_input[0]: \n",
    "\tsampled_token_index = np.argmax(vec)\n",
    "\tsampled_char = index_to_tar[sampled_token_index] \n",
    "\ttest_dec_names.append(sampled_char) \n",
    "# print(test_dec_names[1:len(target_texts[test_data_num])]) \n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1)) \n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1)) \n",
    "\n",
    "\n",
    "ax.set_yticklabels(['']+test_dec_names[1:] + ['<END>'])\n",
    "ax.set_xticklabels(['']+test_enc_names) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUlvYU_U29dY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "위키독스 통쨰 테스트2.ipynb의 사본",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
