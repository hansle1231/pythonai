{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requirement \n",
    "\n",
    "selenium\n",
    "\n",
    "chrome_driver\n",
    "\n",
    "chrome_driver설치 : https://chromedriver.chromium.org/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동형이음어 크롤러\n",
    "\n",
    "네이버-뉴스,블로그, 포스트에서 크롤링\n",
    "\n",
    "다음-뉴스,블로그에서 크롤링\n",
    "\n",
    "* 포털들은 검색어 하나당 제공 게시물을 1000개로 제한한다.\n",
    "\n",
    ": 기간을 나누거나, 더많은 검색어를 바꾼다.\n",
    "(이 경우에는 검색어를 바꿔도 같은 데이터만 크롤링되어서 기간을 바꾸는 방법으로 진행했다.)\n",
    "\n",
    "2014~2019년 까지 1년단위로 기간나눠 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from selenium import webdriver\n",
    "# driver=webdriver.Chrome(r'C:\\Users\\Affinity\\Downloads\\chromedriver_win32\\chromedriver')\n",
    "class crawling:\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "#     import bs4 \n",
    "#     from bs4 import BeautifulSoup\n",
    "    import urllib.request\n",
    "    from urllib import parse\n",
    "    import time\n",
    "    def __init__(self,word_list):\n",
    "        from selenium import webdriver\n",
    "       \n",
    "        self.word_list=word_list\n",
    "        self.driver=webdriver.Chrome(r'C:\\Users\\Affinity\\Downloads\\chromedriver_win32\\chromedriver')  #크롬 드라이버 위치\n",
    "        \n",
    "        \n",
    "   \n",
    "    def naver_news(self,word_list,driv):\n",
    "        import time\n",
    "        from urllib import parse\n",
    "        texts=[]\n",
    "        driver=driv\n",
    "        for i in word_list[:-1]:\n",
    "            \n",
    "           \n",
    "            for kkk in range(5):\n",
    "                base_url='https://search.naver.com/search.naver?where=news&query={}&sm=tab_srt&sort=1&photo=0&field=0&reporter_article=&pd=3&ds={a}&de={b}&docid=&nso=so%3Add%2Cp%3Afrom{a}to{b}%2Ca%3Aall&mynews=0&refresh_start=0&related=0'\n",
    "                q=20190930-kkk*10000\n",
    "                \n",
    "                url=base_url.format(parse.quote(i),a=str(q-10000),b=str(q))\n",
    "                driver.get(url)\n",
    "\n",
    "\n",
    "                q=0\n",
    "                qq=0\n",
    "                page=1\n",
    "                flag=1\n",
    "                while (flag==1):\n",
    "                    q+=1\n",
    "                    qq+=1\n",
    "                    css=\"#sp_nws{} > dl > dd:nth-child(3)\".format(qq)\n",
    "                    ind=word_list[0].index('(')                \n",
    "\n",
    "\n",
    "                    try:\n",
    "                        a=driver.find_element_by_css_selector(css)\n",
    "                        for w in a.text.split('.'):\n",
    "                            if ' '+word_list[0][:ind] in w:\n",
    "                                texts.append(w)\n",
    "                                print(w)\n",
    "                    except:\n",
    "                        print('네이버뉴스 에러')\n",
    "                    if q%10==0:\n",
    "                        try:\n",
    "                            driver.find_element_by_css_selector('#main_pack > div.news.mynews.section._prs_nws > div.paging > a.next').click()\n",
    "                            time.sleep(0.5) \n",
    "                        except:\n",
    "                            flag=0\n",
    "\n",
    "        return texts\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def deco(self,base_url,base_css,base_css2,word_list,texts,click_css,driver):\n",
    "        from urllib import parse\n",
    "        \n",
    "        for i in word_list[:-1]:\n",
    "            for kkk in range(5):\n",
    "                q=20190930-kkk*10000   \n",
    "                url=base_url.format(parse.quote(i),a=str(q-10000),b=str(q))\n",
    "                driver.get(url)\n",
    "                ind=word_list[0].index('(')\n",
    "                flag=1\n",
    "                print(url)\n",
    "                while (flag==1): \n",
    "\n",
    "                    for w in range(1,11):\n",
    "                        try:\n",
    "                            css=base_css.format(w)\n",
    "\n",
    "                            a=driver.find_element_by_css_selector(css)\n",
    "                            for o in a.text.split('.'):\n",
    "                                if ' '+word_list[0][:ind] in o:\n",
    "                                    texts.append(o)\n",
    "                                    print('nb 본문:',o)\n",
    "                        except:\n",
    "                            print('nb 본문에러')\n",
    "\n",
    "                        if base_css2!=None:\n",
    "\n",
    "                            try:    \n",
    "\n",
    "                                css2=base_css2.format(w)\n",
    "                                b=driver.find_element_by_css_selector(css2)\n",
    "                                c=b.get_attribute('title')\n",
    "                                if ' '+word_list[0][:2] in c:\n",
    "                                    texts.append(c)\n",
    "                                    print('nb 제목:',c)\n",
    "                            except:\n",
    "                                print('nb 제목에러')\n",
    "\n",
    "\n",
    "                    try:\n",
    "                        driver.find_element_by_css_selector(click_css).click()\n",
    "                    except:\n",
    "                        flag=0\n",
    "            return texts\n",
    "\n",
    "    \n",
    "    def naver_blog(self,word_list,driv):\n",
    "        from urllib import parse\n",
    "        driver=driv\n",
    "    \n",
    "        texts=self.naver_news(word_list,driver)#self.naver_news(word_list,driver)\n",
    "        base_url='https://search.naver.com/search.naver?where=post&query={}&st=sim&sm=tab_opt&date_from={a}&date_to={b}&date_option=8&srchby=all&dup_remove=1&post_blogurl=&post_blogurl_without=&nso=so%3Ar%2Ca%3Aall%2Cp%3Afrom{a}to{b}'\n",
    "        base_css='#sp_blog_{} > dl > dd.sh_blog_passage'\n",
    "        base_css2='#sp_blog_{} > dl > dt > a'\n",
    "        click_css='#main_pack > div.paging > a.next'\n",
    "        self.deco(base_url,base_css,base_css2,word_list,texts,click_css,driver)\n",
    "        return texts\n",
    "    \n",
    "    \n",
    "    \n",
    "    def daum_news(self,word_list,driv):\n",
    "        from urllib import parse\n",
    "        driver=driv\n",
    "        texts=self.naver_blog(word_list,driver)\n",
    "        base_url='https://search.daum.net/search?w=news&nil_search=btn&DA=STC&enc=utf8&cluster=y&cluster_page=1&q={}&sd={a}000000&ed={b}235959&period=u'\n",
    "        base_css='#clusterResultUL > li:nth-child({}) > div.wrap_cont > div > p'\n",
    "        click_css='#pagingArea > span > span:nth-child(3) > a'\n",
    "        base_css2=None\n",
    "        self.deco(base_url,base_css,base_css2,word_list,texts,click_css,driver)\n",
    "        return texts\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def daum_blog(self,word_list, driv):\n",
    "        from urllib import parse\n",
    "        driver=driv\n",
    "        texts=self.daum_news(word_list,driver)#self.daum_news(word_list,driv)\n",
    "        base_url='https://search.daum.net/search?w=blog&nil_search=btn&DA=STC&enc=utf8&q={}&sd={a}000000&ed={b}235959&period=u'\n",
    "        base_css='#blogColl > div.coll_cont > ul>li:nth-child({})> div.wrap_cont > div > p'\n",
    "        base_css2='#blogColl > div.coll_cont > ul>li:nth-child({})>div.wrap_cont > div > div.wrap_tit.mg_tit'\n",
    "        click_css='#pagingArea > span > span:nth-child(3) > a'\n",
    "        self.deco(base_url,base_css,base_css2,word_list,texts,click_css,driver)\n",
    "        return texts\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def naver_post(self,word_list,driv):\n",
    "        from urllib import parse\n",
    "        driver=driv\n",
    "        texts=self.daum_blog(word_list,driver)\n",
    "        base_url='https://m.post.naver.com/search/post.nhn?keyword={}'\n",
    "        base_css1='#el_list_container > ul>li:nth-child({})> div > div.feed_body > div.text_area > a.link_end > p'\n",
    "        base_css1_='#_list_container > ul:nth-child({})>li:nth-child({})> div > div.feed_body > div.text_area > a.link_end > p'\n",
    "        base_css2='#el_list_container > ul>li:nth-child({})> div > div.feed_body > div.text_area > a.link_end > strong'\n",
    "        base_css2_='#_list_container > ul:nth-child({})>li:nth-child({})> div > div.feed_body > div.text_area > a.link_end > strong'\n",
    "        for i in word_list[:-1]:\n",
    "            url=base_url.format(parse.quote(i))\n",
    "            driver.get(url)\n",
    "            ind=word_list[0].index('(')\n",
    "            flag=1\n",
    "            print(url)\n",
    "\n",
    "\n",
    "\n",
    "            for k in range(1,21):\n",
    "\n",
    "                driver.execute_script('window.scrollTo(0,999999999999999);')\n",
    "\n",
    "\n",
    "\n",
    "                css=base_css1.format(k)\n",
    "                try:\n",
    "                    a=driver.find_element_by_css_selector(css)\n",
    "                except:\n",
    "                    continue\n",
    "                for o in a.text.split('.'):\n",
    "                    if ' '+word_list[0][:ind] in o:\n",
    "                                texts.append(o)\n",
    "                                print('nb 본문:',o)\n",
    "\n",
    "            driver.execute_script('window.scrollTo(0,999999999999999);')\n",
    "            \n",
    "            w=4\n",
    "            for j in range(20):\n",
    "\n",
    "                for k in range(1,21):\n",
    "\n",
    "                    css=base_css1_.format(w,k)\n",
    "                    try:\n",
    "                        a=driver.find_element_by_css_selector(css)\n",
    "                    except:\n",
    "                        continue\n",
    "                    for o in a.text.split('.'):\n",
    "                        if ' '+word_list[0][:ind] in o:\n",
    "                                    texts.append(o)\n",
    "                                    print('nb 본문:',o)\n",
    "                  \n",
    "                try:\n",
    "                    driver.find_element_by_css_selector('#more_btn > button').click()\n",
    "                    time.sleep(2) #인터넷이 크롤러를 못쫒아가면 에러가 생겨서 2초정도 기다려준다.\n",
    "                    driver.execute_script('window.scrollTo(0,999999999999999);')\n",
    "                except:\n",
    "                    continue\n",
    "                w+=1\n",
    "        return texts\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def run_crawling(self):\n",
    "        \n",
    "        return self.naver_post(self.word_list,self.driver)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def make_df(self):\n",
    "        import json\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "\n",
    "\n",
    "        texts_list=self.run_crawling\n",
    "        text=pd.DataFrame(texts_list,columns=['text'])\n",
    "\n",
    "        if self.word_list[-1]==1:\n",
    "            label=pd.DataFrame(np.ones(len(texts_list)),columns=['label'])\n",
    "\n",
    "\n",
    "        if self.word_list[-1]==0:\n",
    "            label=pd.DataFrame(np.zeros(len(texts_list)),columns=['label'])\n",
    "        concat_df=pd.concat([text,label],axis=1)\n",
    "\n",
    "        concat_dict=concat_df.set_index('text')['label'].to_dict()\n",
    "\n",
    "        \n",
    "\n",
    "        return concat_dict,concat_df\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def make_word_list(words):\n",
    "    from jamo import j2hcj,h2j\n",
    "  \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    for q in words:\n",
    "        print('word:',q)\n",
    "        word_list=[]\n",
    "        ind=q[0].index('(')\n",
    "        word_list.append(q[0])\n",
    "#         if len(j2hcj(h2j(q[0][ind-1])))==2:\n",
    "#             for w in ['가','를','의','이다','로']:\n",
    "#                 word_list.append(q[0]+w)\n",
    "                \n",
    "#         if len(j2hcj(h2j(q[0][ind-1])))==3:\n",
    "#             for w in ['이','으로','을','의','이다']:\n",
    "#                 word_list.append(q[0]+w)\n",
    "        \n",
    "        word_list=word_list+[q[1]]\n",
    "        \n",
    "        print(word_list)\n",
    "        \n",
    "        concat_dict,concat_df=crawling(word_list).make_df\n",
    "        print('word_list:',word_list)\n",
    "        \n",
    "        \n",
    "        save_name='D:/df2/{}-{}.csv'.format(word_list[0][:],str(word_list[-1]))\n",
    "        concat_df.to_csv(save_name)\n",
    "      \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용법\n",
    "\n",
    "words=[[word1(관련단어1),1],[word2(관련단어),0],[word2(관련단어),1]]\n",
    "\n",
    "\n",
    "make_word_list(words)\n",
    "\n",
    "### ex)\n",
    "\n",
    "words=[ ['송장(택배)',1] , ['송장(시체)',0] ]\n",
    "\n",
    "\n",
    "make_word_list(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
